# -*- coding: utf-8 -*-
"""Loan Dataset_Coba 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vxNwgOYrq0sj6T2a2dEyADjuadfc2nTA

<hr/>

# Loan Repayment Prediction

<hr/>

Mengingat data historis pinjaman yang diberikan dengan informasi tentang apakah peminjam gagal (charge-off), dapatkah kita membangun model yang dapat memprediksi apakah peminjam akan membayar kembali pinjamannya? Dengan cara ini di masa depan ketika kami mendapatkan pelanggan potensial baru, kami dapat menilai apakah mereka akan membayar kembali pinjamannya atau tidak.

## Table of Contents

* [1. Setup and Import](#1)
* [2. Limit the Feature Space](#2)
 * [2.1 Drop features missing more than 30% data](#2.1)
 * [2.2 Only keep loan features known to potential investors](#2.2)
* [3. Data Analysis and Pre-processing](#3)
 * [3.1 Overlook on Data](#3.1)
 * [3.2  Pre-processing on each Variable](#3.2)
* [4. More Pre-processing](#4)
 * [4.1 Convert loan status to 0/1 charge-off indicator](#4.1)
 * [4.2 Create dummy variables](#4.2)
 * [4.3 Train/test split](#4.3)
* [5. Linear Dependence of Charge-off on the Predictors](#5)
 * [5.1 Pearson correlation](#5.1)
* [6. Model Training and Testing](#6)
 * [6.1 Neural Network](#6.1)
 * [6.2 Logistic regression with SGD training](#6.2)
 * [6.3 Random forest classifier](#6.3)
 * [6.4 Tune hyperparameters on the chosen model more finely](#6.4)
 * [6.5 Test set evaluation](#6.5)
* [7. Conclusion](#7)

<a id="1"></a>
# 1. Setup and Import
<hr>
"""

# Commented out IPython magic to ensure Python compatibility.
# data analysis and wrangling
import pandas as pd
import numpy as np
import random as rnd
import scipy as sp

# visualization
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib as mpl
# %matplotlib inline
mpl.style.use('ggplot')
sns.set(style='whitegrid')

# scaling and train test split
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# pandas options
pd.set_option('display.max_colwidth', 1000, 'display.max_rows', None, 'display.max_columns', None)
pd.set_option("display.precision", 4) # Show numbers only with 2 decimal places

# machine learning algorithm
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import SGDClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import roc_auc_score

# creating a model
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation
from tensorflow.keras.constraints import max_norm
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.models import load_model

# evaluation on test data
from sklearn.metrics import classification_report,confusion_matrix

# Create dataframe from dataset file
loans = pd.read_csv('loan_data_2007_2014_1.csv', index_col=0)
loans.head()

loans.info()

loans.isna().sum().sort_values()

loans.shape

"""<a id="2"></a>
# 2. Limit the Feature Space

The full dataset has 150 features for each loan. We'll select features in two steps:

1. Drop features with more than 30% of their data missing.
2. Of the remaining features, choose only those that would be available to an investor before deciding to fund the loan.

<a id="2.1"></a>
## 2.1 Drop features missing more than 30% data
"""

missing_fractions = loans.isnull().mean().sort_values(ascending=False)
missing_fractions.head(10)

plt.figure(figsize=(6,3), dpi=90)
missing_fractions.plot.hist(bins=20)
plt.title('Histogram of Feature Incompleteness')
plt.xlabel('Fraction of data missing')
plt.ylabel('Feature count')

"""Dari histogram di atas, kami melihat ada kesenjangan besar antara fitur yang tidak memiliki "beberapa" data (<20%) dan yang tidak memiliki "banyak" data (>40%). Karena umumnya sangat sulit untuk secara akurat mengaitkan data dengan lebih dari 30% nilai yang hilang, kami menghapus kolom tersebut. Pertama-tama simpan semua variabel yang kehilangan lebih dari 30% data dalam data frame."""

drop_list = sorted(list(missing_fractions[missing_fractions > 0.3].index))
print(drop_list)
print("\n\n Drop Features: ", len(drop_list))

# Drop these features
loans.drop(labels=drop_list, axis=1, inplace=True)

loans.shape

"""<a id="2.2"></a>
## 2.2 Only keep loan features known to potential investors
"""

print(sorted(loans.columns))

"""Untuk masing-masing fitur ini, kami memeriksa deskripsi di Kamus Data dan hanya menyimpan fitur yang akan tersedia bagi investor yang mempertimbangkan investasi dalam pinjaman. Ini termasuk fitur dalam aplikasi pinjaman, dan fitur apa pun yang ditambahkan oleh LendingClub ketika daftar pinjaman diterima, seperti peringkat pinjaman dan tingkat bunga.

Saya menggunakan pengetahuan terbaik yang tersedia untuk menentukan fitur pinjaman mana yang diketahui oleh calon investor. Saya bukan investor di LendingClub, jadi pengetahuan saya tentang proses investasi LendingClub tidak tepat. Jika ragu, saya salah dalam menjatuhkan fitur tersebut.
"""

keep_list = ['addr_state', 'annual_inc', 'application_type', 'dti', 'earliest_cr_line', 'emp_length', 'emp_title', 'fico_range_high', 'fico_range_low', 'grade', 'home_ownership', 'id', 'initial_list_status', 'installment', 'int_rate', 'issue_d', 'loan_amnt', 'loan_status', 'mort_acc', 'open_acc', 'pub_rec', 'pub_rec_bankruptcies', 'purpose', 'revol_bal', 'revol_util', 'sub_grade', 'term', 'title', 'total_acc', 'verification_status', 'zip_code']
print("Keep features: ", len(keep_list))

drop_list = [col for col in loans.columns if col not in keep_list]
print(drop_list)

print("\n\nNumber of features needed to be dropped: ",len(drop_list))

loans.drop(labels=drop_list, axis=1, inplace=True)
loans.shape

"""<a id="3"></a>
# 3. Data Analysis and Pre-processing

* Overlook on Data
* Pre-processing on each Variable

<a id="3.1"></a>
## 3.1 Overlook on Data
"""

print("Records: ", loans.shape[0], "\nFeatures: ", loans.shape[1])
print("\nInformation of Dataset\n-------")
loans.info()

loans.describe(include=np.object)

# Target Variable
loans['loan_status'].value_counts(dropna=False)

"""Kami mencoba mempelajari perbedaan fitur antara pinjaman yang telah dilunasi (fully paid) atau dilunasi (charge off). Kami tidak akan mempertimbangkan pinjaman yang lancar (current), tidak memenuhi kebijakan kredit (don't meet the credit policy), gagal bayar (default), atau statusnya hilang. Jadi kami hanya menyimpan pinjaman dengan status "Fully Paid" atau "Charged off"."""

loans = loans.loc[loans['loan_status'].isin(['Fully Paid', 'Charged Off'])]

sns.countplot(x='loan_status',data=loans)

# Categories in percentage
loans['loan_status'].value_counts(normalize=True, dropna=False)*100

"""* Ini adalah masalah ketidakseimbangan, karena kita memiliki lebih banyak entri orang yang melunasi pinjaman mereka daripada orang yang tidak membayar kembali.
* Kami dapat berharap untuk mungkin melakukannya dengan sangat baik dalam hal akurasi tetapi presisi dan recall kami akan menjadi metrik sebenarnya yang harus kami evaluasi berdasarkan model.

<a id="3.2"></a>
## 3.2 Pre-processing on each Variable

Kami akan memeriksa setiap fitur satu per satu, dan melakukan hal berikut:

1. Drop fitur jika tidak digunakan untuk prediksi status peminjaman.
2. Lihat statistik ringkasan dan visualisasikan data, dengan plot terhadap status pinjaman.
3. Ubah fitur agar berguna untuk pemodelan, jika perlu.

Kami mendefinisikan fungsi untuk memplot variabel dan membandingkan dengan status pinjaman:
"""

def plot_var(col_name, full_name, continuous):
    """
    Visualize a variable with and without faceting on the loan status.
    - col_name is the variable name in the dataframe
    - full_name is the full variable name
    - continuous is True if the variable is continuous, False otherwise
    """
    f, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12,3), dpi=90)
    
    # Plot without loan status
    if continuous:
        sns.distplot(loans.loc[loans[col_name].notnull(), col_name], kde=False, ax=ax1)
    else:
        sns.countplot(loans[col_name], order=sorted(loans[col_name].unique()), color='#5975A4', saturation=1, ax=ax1)
    ax1.set_xlabel(full_name)
    ax1.set_ylabel('Count')
    ax1.set_title(full_name)

    # Plot with loan status
    if continuous:
        sns.boxplot(x=col_name, y='loan_status', data=loans, ax=ax2)
        ax2.set_ylabel('')
        ax2.set_title(full_name + ' by Loan Status')
    else:
        charge_off_rates = loans.groupby(col_name)['loan_status'].value_counts(normalize=True).loc[:,'Charged Off']
        sns.barplot(x=charge_off_rates.index, y=charge_off_rates.values, color='#5975A4', saturation=1, ax=ax2)
        ax2.set_ylabel('Fraction of Loans Charged-off')
        ax2.set_title('Charge-off Rate by ' + full_name)
    ax2.set_xlabel(full_name)
    
    plt.tight_layout()

"""Print the remaining features for future reference:"""

print(list(loans.columns))

"""### 3.2.1 id

Data Dictionary: "ID yang ditugaskan LC yang unik untuk daftar pinjaman."

Are all the IDs unique?
"""

loans['id'].describe()

"""Ya, mereka semua unik. ID tidak berguna untuk pemodelan, baik sebagai variabel kategori (terlalu banyak nilai yang berbeda) atau sebagai variabel numerik (ID sangat bervariasi besarnya, kemungkinan tanpa signifikansi apa pun), jadi kami menghapus variabel ini."""

loans.drop('id', axis=1, inplace=True)

"""###3.2.2 Correlation"""

plt.figure(figsize=(12, 8))
sns.heatmap(loans.corr(), annot=True, cmap='viridis')

"""Kami melihat korelasi yang hampir sempurna antara fitur "loan_amnt" dengan "installment". Kami akan mengeksplorasi fitur ini lebih lanjut. Cetak deskripsi mereka dan lakukan scatterplot di antara mereka.

* Apakah hubungan ini masuk akal bagi Anda?
* Apakah menurut kami ada informasi duplikat di sini?

### 3.2.3 loan_amnt

Data Dictionary: "Jumlah pinjaman yang terdaftar yang diterapkan oleh peminjam. Jika pada suatu titik waktu, departemen mengurangi jumlah pinjaman, maka itu akan tercermin dalam nilai."
"""

loans['loan_amnt'].describe()

"""Loan amounts range from \$500 to \$35,000, with a median of \$12,000."""

plot_var('loan_amnt', 'Loan Amount', continuous=True)

"""Charged-off loans tend to have higher loan amounts. Let's compare the summary statistics by loan status:"""

loans.groupby('loan_status')['loan_amnt'].describe()

"""### 3.2.4 installment
<a id="3.2.4"></a>

Data Dictionary: "Pembayaran bulanan terutang oleh peminjam jika pinjaman berasal."
"""

loans['installment'].describe()

"""Installments range from \$15.69 to \$1,408, with a median of \$365."""

plot_var('installment', 'Installment', continuous=True)

"""Charged-off loans tend to have higher installments. Let's compare the summary statistics by loan status:"""

loans.groupby('loan_status')['installment'].describe()

"""Pinjaman yang dikenakan biaya rata-rata memiliki cicilan $30 lebih tinggi.

### 3.2.5 term

Data Dictionary: "Jumlah pembayaran pinjaman. Nilai dalam bulan dan dapat berupa 36 atau 60."
"""

loans['term'].value_counts(dropna=False)

"""Convert `term` to integers."""

loans['term'] = loans['term'].apply(lambda s: np.int8(s.split()[0]))
loans['term'].value_counts(normalize=True)

"""Compare the charge-off rate by loan period:"""

loans.groupby('term')['loan_status'].value_counts(normalize=True).loc[:,'Charged Off']

"""Sekitar 78% dari pinjaman yang diselesaikan memiliki jangka waktu tiga tahun, dan sisanya memiliki jangka waktu lima tahun. Pinjaman dengan periode lima tahun lebih dari dua kali lebih mungkin untuk ditagih daripada pinjaman dengan periode tiga tahun.

### 3.2.6 int_rate

Data Dictionary: "Suku bunga pinjaman."
"""

loans['int_rate'].describe()

"""Interest rates range from 5.42% to 26% (!) with a median of 13.6%."""

plot_var('int_rate', 'Interest Rate', continuous=True)

"""Charged-off loans tend to have much higher interest rates. Let's compare the summary statistics by loan status:"""

loans.groupby('loan_status')['int_rate'].describe()

"""### 3.2.7 grade, sub_grade

Data Dictionary for `grade`: "LendingClub menetapkan peringkat pinjaman."

Data Dictionary for `sub_grade`: "LendingClub menetapkan subgrade pinjaman."

Apa kemungkinan nilai `grade` dan `sub_grade`?
"""

print(sorted(loans['grade'].unique()))

print(sorted(loans['sub_grade'].unique()))

"""The grade is implied by the subgrade, so let's drop the grade column."""

loans.drop('grade', axis=1, inplace=True)

plot_var('sub_grade', 'Subgrade', continuous=False)

"""Ada kecenderungan yang jelas dari kemungkinan charge-off yang lebih tinggi karena sub-grade memburuk.

### 3.2.8 emp_title

Data Dictionary: "Jabatan yang diberikan oleh Peminjam saat mengajukan pinjaman."
"""

loans['emp_title'].describe()

"""Ada terlalu banyak judul pekerjaan yang berbeda agar fitur ini tidak berguna, jadi harus di drop."""

loans.drop(labels='emp_title', axis=1, inplace=True)

"""### 3.2.9 emp_length

Data Dictionary: "Lama kerja dalam tahun. Nilai yang mungkin antara 0 dan 10 di mana 0 berarti kurang dari satu tahun dan 10 berarti sepuluh tahun atau lebih." Data sebenarnya tidak cocok dengan deskripsi ini:
"""

loans['emp_length'].value_counts(dropna=False).sort_index()

"""Perhatikan: ada 8.673 pinjaman tanpa data masa kerja (NaN).

Convert `emp_length` to integers:
"""

loans['emp_length'].replace(to_replace='10+ years', value='10 years', inplace=True)

loans['emp_length'].replace('< 1 year', '0 years', inplace=True)

def emp_length_to_int(s):
    if pd.isnull(s):
        return s
    else:
        return np.int8(s.split()[0])

loans['emp_length'] = loans['emp_length'].apply(emp_length_to_int)

loans['emp_length'].value_counts(dropna=False).sort_index()

plot_var('emp_length', 'Employment Length', continuous=False)

loans.drop('emp_length', axis=1, inplace=True)

"""Status pinjaman tampaknya tidak jauh berbeda dengan rata-rata lama kerja, kecuali sedikit penurunan biaya untuk peminjam dengan masa kerja lebih dari 10 tahun.

### 3.2.10 home_ownership

Data Dictionary: "Status kepemilikan rumah yang diberikan oleh peminjam pada saat pendaftaran atau diperoleh dari laporan kredit. kategori nilai: RENT, OWN, MORTGAGE, OTHER."
"""

loans['home_ownership'].value_counts(dropna=False)

"""Mengganti nilai `ANY` dan `NONE` dengan `OTHER`:"""

loans['home_ownership'].replace(['NONE', 'ANY'], 'OTHER', inplace=True)

loans['home_ownership'].value_counts(dropna=False)

plot_var('home_ownership', 'Home Ownership', continuous=False)

"""Tampaknya ada perbedaan besar dalam tarif charge-off berdasarkan status kepemilikan rumah. Penyewa dan pemilik rumah memiliki kemungkinan charge-off yang lebih tinggi. Mari kita bandingkan tarif charge-off:"""

loans.groupby('home_ownership')['loan_status'].value_counts(normalize=True).loc[:,'Charged Off']

"""### 3.2.11 annual_inc

Data Dictionary: "Pendapatan tahunan yang dilaporkan sendiri yang disediakan oleh peminjam selama pendaftaran."
"""

loans['annual_inc'].describe()

"""Pendapatan tahunan berkisar dari \$3 hingga \$7.141.000, dengan median \$62.000. Karena rentang pendapatan yang besar, kita harus mengambil transformasi log dari variabel pendapatan tahunan."""

loans['log_annual_inc'] = loans['annual_inc'].apply(lambda x: np.log10(x+1))

loans.drop('annual_inc', axis=1, inplace=True)

loans['log_annual_inc'].describe()

plot_var('log_annual_inc', 'Log Annual Income', continuous=True)

"""It appears that individuals with higher income are more likely to pay off their loans. Let's compare the summary statistics by loan status:"""

loans.groupby('loan_status')['log_annual_inc'].describe()

"""### 3.2.12 verification_status

Data Dictionary: "Menunjukkan jika pendapatan diverifikasi oleh [Lending Club], tidak diverifikasi, atau jika sumber pendapatan diverifikasi."
"""

plot_var('verification_status', 'Verification Status', continuous=False)

"""### 3.2.13 issue_d

Data Dictionary: "Bulan di mana pinjaman itu didanai."




Karena kami hanya menggunakan variabel yang tersedia untuk investor sebelum pinjaman didanai, `issue_d` tidak akan disertakan dalam model akhir. Kami menyimpannya untuk saat ini hanya untuk melakukan train/tes split nanti, lalu kami akan drop.

### 3.2.14 purpose

Data Dictionary: "Kategori yang disediakan oleh peminjam untuk permintaan pinjaman."
"""

loans['purpose'].value_counts()

"""Calculate the charge-off rates by purpose:"""

loans.groupby('purpose')['loan_status'].value_counts(normalize=True).loc[:,'Charged Off'].sort_values()

"""Perhatikan bahwa hanya 12% dari pinjaman yang telah diselesaikan untuk pernikahan yang telah ditagih, tetapi 30% dari pinjaman usaha kecil yang telah diselesaikan telah ditagih.

### 3.2.15 title

Data Dictionary: "Judul pinjaman yang diberikan oleh peminjam."
"""

loans['title'].describe()

"""View the top 10 loan titles, and their frequencies:"""

loans['title'].value_counts().head(10)

"""Ada 55995 judul berbeda dalam kumpulan data, dan berdasarkan 10 judul teratas, variabel `purpose` tampaknya sudah berisi informasi ini. Jadi kita drop variabel `title`."""

loans.drop('title', axis=1, inplace=True)

"""### 3.2.16 zip-code, addr_state

Data Dictionary for `zip_code`: "3 angka pertama kode pos yang diberikan oleh peminjam dalam aplikasi pinjaman." (Sudah di drop dari data frame)

Data Dictionary for `addr_state`: "Negara yang disediakan oleh peminjam dalam aplikasi pinjaman."
"""

loans['zip_code'].sample(5)

loans['zip_code'].nunique()

loans['addr_state'].sample(5)

loans.drop(labels='zip_code', axis=1, inplace=True)

loans['addr_state'].nunique()

"""Calculate the charge-off rates by address state:"""

loans.groupby('addr_state')['loan_status'].value_counts(normalize=True).loc[:,'Charged Off'].sort_values()

"""The charge-off rate ranges from 10.5% in Washington, DC to 26.6% in Mississippi.

### 3.2.17 dti

Data Dictionary: "Rasio yang dihitung menggunakan total pembayaran utang bulanan peminjam atas total kewajiban utang, tidak termasuk hipotek dan pinjaman LC yang diminta, dibagi dengan pendapatan bulanan yang dilaporkan sendiri oleh peminjam."
"""

loans['dti'].describe()

"""Debt-to-income from 0 to 39.99, with a median of 16.05.

"""

plt.figure(figsize=(8,3), dpi=90)
sns.distplot(loans.loc[loans['dti'].notnull(), 'dti'], kde=False)
plt.xlabel('Debt-to-income Ratio')
plt.ylabel('Count')
plt.title('Debt-to-income Ratio')

"""Very few. Compare the summary statistics by loan status:"""

loans.groupby('loan_status')['dti'].describe()

"""Completed loans that are charged off tend to have higher debt-to-income ratios.

### 3.2.18 earliest_cr_line

Data Dictionary: "Bulan dimana batas kredit paling awal yang dilaporkan peminjam dibuka."
"""

loans['earliest_cr_line'].sample(5)

loans['earliest_cr_line'].isnull().any()

"""Let's just retain the year for simplicity:"""

loans['earliest_cr_line'] = loans['earliest_cr_line'].apply(lambda s: int(s[-4:]))

loans['earliest_cr_line'].describe()

plot_var('earliest_cr_line', 'Year of Earliest Credit Line', continuous=True)

"""Peminjam yang ditagih cenderung memiliki jalur kredit yang lebih pendek.

### 3.2.19 open_acc

Data Dictionary: "Jumlah jalur kredit terbuka dalam file kredit peminjam."
"""

plt.figure(figsize=(10,3), dpi=90)
sns.countplot(loans['open_acc'], order=sorted(loans['open_acc'].unique()), color='#5975A4', saturation=1)
_, _ = plt.xticks(np.arange(0, 90, 5), np.arange(0, 90, 5))
plt.title('Number of Open Credit Lines')

"""Apakah ada perbedaan jumlah jalur kredit antara pinjaman yang dibayar penuh dan pinjaman yang dibebankan?"""

loans.groupby('loan_status')['open_acc'].describe()

"""### 3.2.20 pub_rec

Data Dictionary: "Number of derogatory public records."
"""

loans['pub_rec'].value_counts().sort_index()

"""Is there a difference in average public records between fully paid loans and charged-off loans?"""

loans.groupby('loan_status')['pub_rec'].describe()

"""### 3.2.21 revol_bal

Data Dictionary: "Total credit revolving balance."
"""

loans['revol_bal'].describe()

"""Do a log transform:"""

loans['log_revol_bal'] = loans['revol_bal'].apply(lambda x: np.log10(x+1))

loans.drop('revol_bal', axis=1, inplace=True)

plot_var('log_revol_bal', 'Log Revolving Credit Balance', continuous=True)

loans.groupby('loan_status')['log_revol_bal'].describe()

"""There isn't a large difference in the means.

### 3.2.22 revol_util

Data Dictionary: "Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit."
"""

# Let's remove % percent sign from `revol_util` and convert it into `float`
loans.revol_util = loans.revol_util.map(lambda x: str(x).replace('%','')).astype(np.float64)
loans['revol_util'].head()

loans['revol_util'].describe()

plot_var('revol_util', 'Revolving Line Utilization', continuous=True)

loans.groupby('loan_status')['revol_util'].describe()

"""### 3.2.23 total_acc

Data Dictionary: "The total number of credit lines currently in the borrower's credit file."
"""

plt.figure(figsize=(12,3), dpi=90)
sns.countplot(loans['total_acc'], order=sorted(loans['total_acc'].unique()), color='#5975A4', saturation=1)
_, _ = plt.xticks(np.arange(0, 176, 10), np.arange(0, 176, 10))
plt.title('Total Number of Credit Lines')

loans.groupby('loan_status')['total_acc'].describe()

"""No large differences here.

### 3.2.24 initial_list_status

Data Dictionary: "The initial listing status of the loan. Possible values are – W, F." I'm not sure what this means.
"""

plot_var('initial_list_status', 'Initial List Status', continuous=False)

"""### 3.2.25 application_type

Data Dictionary: "Indicates whether the loan is an individual application or a joint application with two co-borrowers."
"""

loans['application_type'].value_counts()

"""Let's just compare the charge-off rates by application type:"""

loans.groupby('application_type')['loan_status'].value_counts(normalize=True).loc[:,'Charged Off']

"""Joint loans are slightly less likely to be charged-off.

<a id="4"></a>
# 4. More Pre-processing

<a id="4.1"></a>
## 4.1 Convert loan status to 0/1 charge-off indicator

Change the response variable `loan_status` to a 0/1 variable, where 0 indicates fully paid and 1 indicates charge-off:
"""

loans['charged_off'] = (loans['loan_status'] == 'Charged Off').apply(np.uint8)
loans.drop('loan_status', axis=1, inplace=True)

loans = loans.dropna().reset_index(drop=True)

"""<a id="4.2"></a>
## 4.2 Create dummy variables

How many variables do we currently have?
"""

loans.shape

"""If any categorical variables have missing values, we'll need to create NaN dummy variables for those. So first check which variables have missing data:"""

missing_fractions = loans.isnull().mean().sort_values(ascending=False) # Fraction of data missing for each variable

print(missing_fractions[missing_fractions > 0]) # Print variables that are missing data

"""There are no categorical variables with missing values, and therefore we don't need any `NaN` dummy variables.

Create dummy variables for the categorical variables:
"""

print(loans.columns)

loans = pd.get_dummies(loans, columns=['sub_grade', 'home_ownership', 'verification_status', 'purpose', 'addr_state', 'initial_list_status', 'application_type'], drop_first=True)

"""How many variables are there now?"""

loans.shape

"""Check our data with the new dummy variables:"""

loans.sample(5)

"""<a id="4.3"></a>
## 4.3 Train/test split
"""

# We'll make our modeling problem more realistic by performing the train/test split based on the month that the loan was funded. That is, we'll use loans funded on earlier dates to predict whether future loans will charge-off. The variable `issue_d` includes the month and year that the loan was funded.
# loans['issue_d'].sample(5)

# Are there any missing values?
# loans['issue_d'].isnull().any()

# No. Let's convert the issue dates to datetime objects:
# loans['issue_d'] = pd.to_datetime(loans['issue_d'])
# loans['issue_d'].sample(5)

# The new datetime values are all on the first day of the month. Check the summary statistics of the issue dates:
# loans['issue_d'].describe()
# There are only 154 unique issue dates over the 10-year period because we only have month/year information. In this particular dataset, the first loans were issued in June 2007, and the most recent loans were issued in March 2020. The busiest month was March 2016 with 57,553 loans funded in that month. What is the distribution of loans funded in each year?

# plt.figure(figsize=(6,3), dpi=90)
# loans['issue_d'].dt.year.value_counts().sort_index().plot.bar(color='darkblue')
# plt.xlabel('Year')
# plt.ylabel('Number of Loans Funded')
# plt.title('Loans Funded per Year')

# We'll form the test set from the most recent 10% of the loans.

# loans_train = loans.loc[loans['issue_d'] <  loans['issue_d'].quantile(0.9)]
# loans_test =  loans.loc[loans['issue_d'] >= loans['issue_d'].quantile(0.9)]
# Refer [this video](https://www.youtube.com/watch?v=idXEk3MjC9M) if you didn't understood the quantile. Check that we properly partitioned the loans:

# What is the test size?
# loans_test.shape[0] / loans.shape[0]
# About 10.8%. The partition looks good, so we can delete the original `loans` dataframe:
# The training set includes loans from June 2007 to April 2018. The test set includes loans from May 2018 to March 2020. Now we need to delete the `issue_d` variable, because it was not available before the loan was funded.

# del loans

# loans_train.drop('issue_d', axis=1, inplace=True)
# loans_test.drop('issue_d', axis=1, inplace=True)

# del loans_train, loans_test

# y_train = loans_train['charged_off']
# y_test = loans_test['charged_off']

# X_train = loans_train.drop('charged_off', axis=1)
# X_test = loans_test.drop('charged_off', axis=1)

loans.drop('issue_d', axis=1, inplace=True)

# Features
X = loans.drop('charged_off',axis=1)

# Label
y = loans['charged_off']

# Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)

print("X_train.shape: ", X_train.shape)
print("X_test.shape:  ", X_test.shape)
print("y_train.shape: ", y_train.shape)
print("y_test.shape:  ", y_test.shape)

"""<a id="5"></a>
# 5. Linear Dependence of Charge-off on the Predictors

On the training set, we compute the [Pearson correlation](https://www.youtube.com/watch?v=6fUYt1alA1U), $F$-statistic, and $p$ value of each predictor with the response variable `charged_off`.
"""

linear_dep = pd.DataFrame()

"""<a id="5.1"></a>
## 5.1 Pearson correlation
Kami menggunakan koefisien korelasi Pearson untuk menguji kekuatan dan arah hubungan linier antara dua variabel kontinu.

Koefisien korelasi dapat berkisar dalam nilai dari 1 hingga +1. Semakin besar nilai absolut koefisien, semakin kuat hubungan antar variabel. Untuk korelasi Pearson, nilai mutlak 1 menunjukkan hubungan linier yang sempurna. Korelasi yang mendekati 0 menunjukkan tidak ada hubungan linier antar variabel.

Tanda koefisien menunjukkan arah hubungan. Jika kedua variabel cenderung naik atau turun bersama-sama, koefisiennya positif, dan garis yang mewakili korelasi miring ke atas. Jika satu variabel cenderung meningkat sementara yang lain menurun, koefisiennya negatif, dan garis yang menunjukkan korelasi miring ke bawah.

Kita bisa melihat korelasi yang kuat antara loan_amnt dan cicilan. (Pembayaran bulanan terutang oleh peminjam jika pinjaman berasal)
Selengkapnya tentang teks sumber ini
"""

for col in X_train.columns:
    linear_dep.loc[col, 'pearson_corr'] = X_train[col].corr(y_train)
linear_dep['abs_pearson_corr'] = abs(linear_dep['pearson_corr'])

linear_dep

"""$F$-statistics:"""

from sklearn.feature_selection import f_classif
for col in X_train.columns:
    mask = X_train[col].notnull()
    (linear_dep.loc[col, 'F'], linear_dep.loc[col, 'p_value']) = f_classif(pd.DataFrame(X_train.loc[mask, col]), y_train.loc[mask])

"""Sort the results by the absolute value of the Pearson correlation:"""

linear_dep.sort_values('abs_pearson_corr', ascending=False, inplace=True)
linear_dep.drop('abs_pearson_corr', axis=1, inplace=True)

"""Reset the index:"""

linear_dep.reset_index(inplace=True)
linear_dep.rename(columns={'index':'variable'}, inplace=True)

"""View the results for the top 20 predictors most correlated with `charged_off`:"""

linear_dep.head(30)

"""The variables most linearly correlated with `charged_off` are the interest rate, loan period (term), debt-to-income ratio, Revolving line utilization rate, income, the loan grade, and the loan amount.

Now view the results for the 20 least correlated predictors:
"""

linear_dep.tail(20)

"""It looks like the borrower's other home ownership, several of the address state, and several of the loan purposes are irrelevant for predicting charge-off.

<a id="6"></a>
# 6. Model Training and Testing

We implement machine learning pipelines consisting of one or more of the following steps, depending on the particular model:
1. Mean imputation of missing values
2. Dimension reduction using linear discriminant analysis (LDA)
3. Data standardization: rescaling to zero mean and unit variance
4. The chosen model

We will evaluate and compare the following models using a cross-validated AUROC score on the training set:
1. Neural Network
2. Logistic regression with SGD training
3. Random forest

We'll perform some hyperparameter tuning for each model to choose the most promising model, then more carefully tune the hyperparameters of the best-performing model.

<a id="6.1"></a>
## 6.1 Neural Network
"""

scaler = MinMaxScaler()

# fit and transfrom
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# everything has been scaled between 1 and 0
print('Max: ',X_train.max())
print('Min: ', X_train.min())

model = Sequential()

# input layer
model.add(Dense(X_train.shape[1],activation='relu'))
model.add(Dropout(0.2))

# hidden layer
model.add(Dense(39,activation='relu'))
model.add(Dropout(0.2))

# hidden layer
model.add(Dense(19,activation='relu'))
model.add(Dropout(0.2))

# output layer
model.add(Dense(1, activation='sigmoid'))

# compile model
model.compile(optimizer="adam", loss='binary_crossentropy')

# Early Stopping
early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)

model.fit(x=X_train, 
          y=y_train, 
          epochs=400,
          verbose = 2,
          batch_size=256,
          validation_data=(X_test, y_test),
          callbacks=[early_stop])

losses = pd.DataFrame(model.history.history)

predictions = (model.predict(X_test) > 0.5).astype("int32")

print('Classification Report:')
print(classification_report(y_test, predictions))
print('\n')
print('Confusion Matirx:')
print(confusion_matrix(y_test, predictions))

rnd.seed(101)
random_ind = rnd.randint(0,len(loans))

new_customer = loans.drop('charged_off',axis=1).iloc[random_ind]
new_customer

# we need to reshape this to be in the same shape of the training data that the model was trained on
#model.predict_classes(new_customer.values.reshape(1,X_train.shape[1]))

predict_x=model.predict(X_test) 
classes_x=np.argmax(predict_x,axis=1)

# the prediction was right
loans.iloc[random_ind]['charged_off']

"""<a id="6.2"></a>
## 6.2 Logistic regression with SGD training

The `SGDClassifier` estimator in scikit-learn implements linear classifiers (SVM, logistic regression, and others) with stochastic gradient descent (SGD) training. A particular linear classifier is chosen through the `loss` hyperparameter. Because we want to predict the probability of charge-off, we choose logistic regression (a probabilistic classifier) by setting `loss = 'log'`.

The machine learning pipeline:
"""

pipeline_sgdlogreg = Pipeline([
    ('imputer', SimpleImputer(copy=False)), # Mean imputation by default
    ('scaler', StandardScaler(copy=False)),
    ('model', SGDClassifier(loss='log', max_iter=1000, tol=1e-3, random_state=1, warm_start=True))
])

"""A small grid of hyperparameters to search over:"""

param_grid_sgdlogreg = {
    'model__alpha': [10**-5, 10**-2, 10**1],
    'model__penalty': ['l1', 'l2']
}

"""Create the search grid object:"""

grid_sgdlogreg = GridSearchCV(estimator=pipeline_sgdlogreg, param_grid=param_grid_sgdlogreg, scoring='roc_auc', n_jobs=1, pre_dispatch=1, cv=5, verbose=1, return_train_score=False)

"""Conduct the grid search and train the final model on the whole dataset:"""

grid_sgdlogreg.fit(X_train, y_train)

"""Mean cross-validated AUROC score of the best model:"""

grid_sgdlogreg.best_score_

"""Best hyperparameters:"""

grid_sgdlogreg.best_params_

"""<a id="6.3"></a>
## 6.3 Random forest classifier

Next we train a random forest model. Note that data standardization is not necessary for a random forest.
"""

pipeline_rfc = Pipeline([
    ('imputer', SimpleImputer(copy=False)),
    ('model', RandomForestClassifier(n_jobs=-1, random_state=1))
])

"""The random forest takes very long to train, so we don't test different hyperparameter choices. We'll still use `GridSearchCV` for the sake of consistency."""

param_grid_rfc = {
    'model__n_estimators': [50] # The number of randomized trees to build
}

"""The AUROC will always improve (with decreasing gains) as the number of estimators increases, but it's not necessarily worth the extra training time and model complexity."""

grid_rfc = GridSearchCV(estimator=pipeline_rfc, param_grid=param_grid_rfc, scoring='roc_auc', n_jobs=1, pre_dispatch=1, cv=5, verbose=1, return_train_score=False)

grid_rfc.fit(X_train, y_train)

"""Mean cross-validated AUROC score of the random forest:"""

grid_rfc.best_score_

"""Not quite as good as logistic regression, at least according to this metric.

<a id="6.4"></a>
## 6.4 Tune hyperparameters on the chosen model more finely

The three models performed quite similarly according to the AUROC:
"""

print('Cross-validated AUROC scores')
print(grid_sgdlogreg.best_score_, '- Logistic regression')
print(grid_rfc.best_score_, '- Random forest')

"""Logistic regression squeaked out ahead, and coupled with the fact that `SGDClassifier` trains much faster than the other two models, we'll select logistic regression as our final model. Now we'll tune the hyperparameters more finely."""

param_grid_sgdlogreg = {
    'model__alpha': np.logspace(-4.5, 0.5, 11), # Fills in the gaps between 10^-5 and 10^1
    'model__penalty': ['l1', 'l2']
}

print(param_grid_sgdlogreg)

grid_sgdlogreg = GridSearchCV(estimator=pipeline_sgdlogreg, param_grid=param_grid_sgdlogreg, scoring='roc_auc', n_jobs=1, pre_dispatch=1, cv=5, verbose=1, return_train_score=False)

grid_sgdlogreg.fit(X_train, y_train)

"""Mean cross-validated AUROC score of the best model:"""

grid_sgdlogreg.best_score_

"""Best hyperparameters:"""

grid_sgdlogreg.best_params_

"""By some coincidence, the optimal hyperparameters here are the same as from our first grid search for logistic regression!

<a id="6.5"></a>
## 6.5 Test set evaluation

Now we can finally see how our chosen model performs on the test data (the most recent 10% of the loans).
"""

y_score = grid_sgdlogreg.predict_proba(X_test)[:,1]
roc_auc_score(y_test, y_score)

"""The test set AUROC score is somewhat lower than the cross-validated score (0.694).

<a id="7"></a>
# 7. Conclusion

Kami menerapkan metode machine learning untuk memprediksi kemungkinan pinjaman yang diminta di perusahaan akan dilunasi. Setelah melatih dan mengevaluasi tiga model yang berbeda (regresi logistik, random forest, dan k-nearest neighbors), kami menemukan bahwa ketiganya melakukan hal yang sama menurut cross-validated AUROC pada data training. Kami memilih regresi logistik (dengan ridge penalty) karena ini adalah model tercepat untuk training, dan model ini memperoleh skor AUROC 0,694 pada set pengujian yang terdiri dari 10% pinjaman terbaru.

Model ini, meskipun jauh dari sempurna, dapat memberikan prediksi yang agak terinformasi tentang kemungkinan pinjaman akan dilunasi, hanya dengan menggunakan data yang tersedia bagi calon investor sebelum pinjaman didanai sepenuhnya.

Kami juga menemukan bahwa, menurut ukuran korelasi linier antara prediktor dan respons, variabel terpenting untuk memprediksi charge-off adalah interest rate dan term, revolving utilization dan debt-to-income ratio.
"""